---
title: "Class 7: Machine Learning I"
author: "Isabel Hui - A16887852"
format: html
---

Today we are going to learn how to apply different machine learning methods, beginning with clustering.

The goal here is to find groups/clusters in your input data.

First I will make up some data with clear gorups. For this I will use the `rnorm()` function:

```{r}
hist(rnorm(10000, mean = 3))
```

We can made two peaks on the plot by concatonating `c()` two `rnorm()` functions like so:

```{r}
n <- 10000
x <- c(rnorm(n, -3), rnorm(n, +3))
hist(x)
```
We can also make a cluster plot to show the groupings along axes that are both (+) and (-).

```{r}
n <- 30
x <- c(rnorm(n, -3), rnorm(n, +3))
y <- rev(x)

z <- cbind(x, y)
head(z)
```
```{r}
plot(z)
```

Use the `kmeans()` function setting k to 2 and `nstart = 20`.

Inspect/print the results.

> Q. How many points are in each cluster?

> Q. What 'component' of your result object details
      - cluster size?
      - cluster assignment/membership?
      - cluster center?
      
> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points.


```{r}
km <- kmeans(z, centers = 2)
km
```

Results in kmenas object `km`.
```{r}
attributes(km)
```

Cluster size?

```{r}
km$size
```

Cluster assignment/membership?

```{r}
km$cluster
```

Cluster center?

```{r}
km$centers
```

> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points.

```{r}
plot(z, col = "red")
```

R will recycle the shorter color vector to be the same length as the longer (length of data points) in z.
```{r}
plot(z, col = c("red", "blue"))
```

R corresponds numbers to colors.
```{r}
plot(z, col = (4))
```

```{r}
plot(z, col = km$cluster)
```

We can use the `points()` function to add new points to an existing plot... like the cluster centers. We can change the color using `col`, shape using `pch`, and size using `cex`.

```{r}
plot(z, col = km$cluster)
points(km$centers, col = "blue", pch = 4, cex = 2)
```
> Q. Can you run `kmeans` and ask for 4 clusters and plot the results like we have done before?

Note that running this multiple times results in a different result each time; it just runs again and again, only make two clusters.

```{r}
km4 <- kmeans(z, centers = 4)
plot(z, col = km4$cluster)
points(km4$centers, col = "blue", pch = 4, cex = 2)
```

## Hierarchical Clustering

Let's take our same made-up data `z` and see how hclust works.

First we need a distance matrix of our data to be clustered.

```{r}
d <- dist(z)
hc <- hclust(d)
hc
```
```{r}
plot(hc)
abline(h=8, col="red")
cutree(hc, h=8)
```
I can get my cluster membership vector by "cutting the tree" with the `cutre()` function like so:

```{r}
grps <- cutree(hc, h=8)
grps
```

Can you plot `z` colored by our hclust results.

```{r}
plot(z, col=grps)
```

## PCA of UK Food Data

Read data from the UK on food consumption in different parts of the UK.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

A so-called "Pairs" plot can be useful for small datasets like this one.

```{r}
pairs(x, col=rainbow(10), pch=16)
```

It is hard to see structure and trends in even this small dataset. How will we ever do this when we big datasets with 1,000s or 10s or thousands of things we are measuring...


## PCA to the rescue

Let's see how PCA deals with this dataset. So main function in base R to do PCA is called `prcromp()`.

We can take the transpose of the data, flipping so the columns are foods using `t()`.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

Let's see what is inside this `pca` object that we created from running `prcomp()`.

```{r}
attributes(pca)
```
```{r}
pca$x
```

```{r}
plot(pca$x[,1], pca$x[,2], col=c("black", "red", "blue", "darkgreen"), pch=16,
     xlab="PC1 (67.4%)", ylab="PC2 (29%)")
```

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```







